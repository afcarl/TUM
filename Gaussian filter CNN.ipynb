{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from skimage import io, transform\n",
    "import scipy.misc as m\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.data import Dataset, Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5750\n"
     ]
    }
   ],
   "source": [
    "# Preparing the dataset\n",
    "data_dir = '/Users/navneetmkumar/Desktop/TUM/lfw'\n",
    "users = os.listdir(data_dir)\n",
    "print(len(users))\n",
    "users = sorted(users[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root_dir = '/Users/navneetmkumar/Desktop/TUM/'\n",
    "image_dir = root_dir+ 'lfw/'\n",
    "\n",
    "if not os.path.exists(root_dir+'train-inputs'):\n",
    "    os.makedirs(root_dir+'train-inputs')\n",
    "\n",
    "if not os.path.exists(root_dir+'train-targets'):\n",
    "    os.makedirs(root_dir+'train-targets')\n",
    "# Getting the data into a single training folder\n",
    "def create_training_set(users, root_dir, image_dir):\n",
    "    for user in users[1:]:\n",
    "        path = os.path.join(image_dir, user)\n",
    "        for f in os.listdir(path):\n",
    "            f_name = os.path.basename(f)\n",
    "            f_name = f_name.split(\".\")[0]\n",
    "            f = os.path.join(path, f)\n",
    "            shutil.copy2(f, root_dir+'train-inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_training_set(users, root_dir, image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = root_dir+'train-inputs/'\n",
    "# Creating the targets dataset\n",
    "def create_targets_set(train_dir):\n",
    "    for f in sorted(os.listdir(train_dir)):\n",
    "        f_name = os.path.join(train_dir+f)\n",
    "        img = io.imread(f_name)\n",
    "        # Make the transformation (Gaussian Blur)\n",
    "        blur = cv2.GaussianBlur(img, (7,7), 0)\n",
    "        io.imsave(root_dir+'train-targets/'+f,blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_targets_set(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13232\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(train_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13232\n"
     ]
    }
   ],
   "source": [
    "train_targets_dir = root_dir+'train-targets/'\n",
    "print(len(os.listdir(train_targets_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_imgs = os.listdir(train_dir)\n",
    "train_targets = os.listdir(train_targets_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the global variables\n",
    "IMG_SIZE = 250\n",
    "# Constants describing the training process.\n",
    "MOVING_AVERAGE_DECAY = 0.9999     \n",
    "NUM_EPOCHS_PER_DECAY = 350.0      \n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1  \n",
    "INITIAL_LEARNING_RATE = 0.1       \n",
    "\n",
    "# Creating the model\n",
    "\n",
    "batch_size= 128\n",
    "\n",
    "def GaussianFilterModel(data):\n",
    "    \n",
    "    number_of_classes = data.shape[2]\n",
    "    \n",
    "    # conv1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        kernel = tf.get_variable('w1', [5,5,3,512], initializer=tf.truncated_normal_initializer(5e-2), dtype=tf.float32)\n",
    "        conv = tf.nn.conv2d(data, kernel, [1,1,1,1], padding='SAME')\n",
    "        biases = tf.get_variable('b1', [512], initializer=tf.constant_initializer(0.1), dtype=tf.float32)\n",
    "        pre_activation = tf.nn.bias_add(conv, biases)\n",
    "        act = tf.nn.relu(pre_activation, name=scope.name)\n",
    "        \n",
    "        #norm\n",
    "        norm = tf.nn.lrn(act, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75,\n",
    "                    name='norm')\n",
    "        \n",
    "        #pool\n",
    "        pool = tf.nn.max_pool(norm, ksize=[1, 3, 3, 1],\n",
    "                         strides=[1, 2, 2, 1], padding='SAME', name='pool')\n",
    "        \n",
    "    with tf.variable_scope('conv4') as scope:\n",
    "        kernel2 = tf.get_variable('w5', [7, 7, 512, 4096], initializer=tf.truncated_normal_initializer(5e-2), dtype=tf.float32)\n",
    "        conv2 = tf.nn.conv2d(pool, kernel2, [1,1,1,1], padding='SAME')\n",
    "        biases2 = tf.get_variable('b5', [4096], initializer=tf.constant_initializer(0.1), dtype=tf.float32)\n",
    "        pre_activation2 = tf.nn.bias_add(conv2, biases2)\n",
    "        act5 = tf.nn.relu(pre_activation2, name=scope.name)\n",
    "        \n",
    "        kernel4 = tf.get_variable('w7', [1,1,4096, 3], initializer=tf.truncated_normal_initializer(5e-2), dtype=tf.float32)\n",
    "        conv4 = tf.nn.conv2d(act5, kernel4, [1,1,1,1], padding='SAME')\n",
    "        biases4 = tf.get_variable('b7', [3], initializer=tf.constant_initializer(0.1), dtype=tf.float32)\n",
    "        pre_activation4 = tf.nn.bias_add(conv4, biases4)\n",
    "    \n",
    "    with tf.variable_scope('local1') as scope:\n",
    "        output = tf.image.resize_images(pre_activation4, tf.Variable([250, 250], tf.int32))\n",
    "        \n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#validation split\n",
    "validation_size = 0.2\n",
    "# Input data\n",
    "x = tf.placeholder(tf.float32, shape=[None, IMG_SIZE, IMG_SIZE, 3], name='x')\n",
    "y = tf.placeholder(tf.float32, shape=[None, IMG_SIZE, IMG_SIZE, 3], name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images = []\n",
    "train_targets = []\n",
    "for f in train_imgs:\n",
    "    f_name = train_dir+f\n",
    "    train_images.append(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for f in train_targets:\n",
    "    f_name = train_targets_dir + f\n",
    "    train_labels.append(f_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images = train_images[0:1000]\n",
    "train_labels = train_labels[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def input_parser(image_path, label_path):\n",
    "    img_file = tf.read_file(image_path)\n",
    "    label_file = tf.read_file(label_path)\n",
    "    img_decoded = tf.image.decode_image(img_file, channels=3)\n",
    "    lbl_decoded = tf.image.decode_image(label_file, channels=3)\n",
    "    \n",
    "    return img_decoded, lbl_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tf.uint8, tf.uint8)\n",
      "(TensorShape(None), TensorShape(None))\n"
     ]
    }
   ],
   "source": [
    "print(tr_data.output_types) \n",
    "print(tr_data.output_shapes)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr_data = Dataset.from_tensor_slices((train_images, train_labels))\n",
    "tr_data = tr_data.map(input_parser)\n",
    "tr_data = tr_data.batch(batch_size)\n",
    "# create TensorFlow Iterator object\n",
    "iterator = Iterator.from_structure(tr_data.output_types,\n",
    "                                   tr_data.output_shapes)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "# create two initialization ops to switch between the datasets\n",
    "training_init_op = iterator.make_initializer(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the training \n",
    "def train_model(x, y):\n",
    "    prediction = GaussianFilterModel(x)\n",
    "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=prediction, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 10\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(training_init_op)\n",
    "        \n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    elem = sess.run(next_element)\n",
    "                    epoch_x, epoch_y = elem\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                    epoch_loss += c\n",
    "                    \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
